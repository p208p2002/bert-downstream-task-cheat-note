{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BertForQuestionAnswering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加載"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _check_has_skip_token(check_tokens,skip_tokens):\n",
    "    for check_token in check_tokens:\n",
    "        for skip_token in skip_tokens:\n",
    "            if check_token == skip_token:\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "def _check_segment_type_is_a(start_index,end_index,segment_embeddings):\n",
    "    tag_segment_embeddings = segment_embeddings[start_index]\n",
    "    if 0 in tag_segment_embeddings:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def _get_best_indexes(logits, n_best_size):\n",
    "    \"\"\"Get the n-best logits from a list.\"\"\"\n",
    "    index_and_score = sorted(enumerate(logits), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    best_indexes = []\n",
    "    for i in range(len(index_and_score)):\n",
    "        if i >= n_best_size:\n",
    "            break\n",
    "        best_indexes.append(index_and_score[i][0])\n",
    "    return best_indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0430 03:48:07.135764 140367158015808 file_utils.py:41] PyTorch version 1.3.0+cu100 available.\n",
      "I0430 03:48:07.924898 140367158015808 tokenization_utils.py:420] Model name 'clue/albert_chinese_small' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'clue/albert_chinese_small' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0430 03:48:11.769205 140367158015808 tokenization_utils.py:504] loading file https://s3.amazonaws.com/models.huggingface.co/bert/clue/albert_chinese_small/vocab.txt from cache at /root/.cache/torch/transformers/3eebaeebe62cc5136d3742b1db02e61c6990c77ff83edff476fea14908d0c3c1.9b42061518a39ca00b8b52059fd2bede8daa613f8a8671500e518a8c29de8c00\n",
      "I0430 03:48:11.769852 140367158015808 tokenization_utils.py:504] loading file https://s3.amazonaws.com/models.huggingface.co/bert/clue/albert_chinese_small/added_tokens.json from cache at None\n",
      "I0430 03:48:11.770397 140367158015808 tokenization_utils.py:504] loading file https://s3.amazonaws.com/models.huggingface.co/bert/clue/albert_chinese_small/special_tokens_map.json from cache at None\n",
      "I0430 03:48:11.770950 140367158015808 tokenization_utils.py:504] loading file https://s3.amazonaws.com/models.huggingface.co/bert/clue/albert_chinese_small/tokenizer_config.json from cache at None\n",
      "I0430 03:48:12.747619 140367158015808 configuration_utils.py:283] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/clue/albert_chinese_small/config.json from cache at /root/.cache/torch/transformers/c0632c9563dabbb04face1cce90c1bcce07a5a280785afcb51be5598a41fbfbc.24a1fa756231f74c40f5d6ab9bf2759ebe676210f1e31a2cbd537190a1bba42e\n",
      "I0430 03:48:12.748529 140367158015808 configuration_utils.py:319] Model config AlbertConfig {\n",
      "  \"_num_labels\": 2,\n",
      "  \"architectures\": null,\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"bad_words_ids\": null,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"classifier_dropout_prob\": 0.1,\n",
      "  \"decoder_start_token_id\": null,\n",
      "  \"do_sample\": false,\n",
      "  \"down_scale_factor\": 1,\n",
      "  \"early_stopping\": false,\n",
      "  \"embedding_size\": 128,\n",
      "  \"eos_token_id\": 3,\n",
      "  \"finetuning_task\": null,\n",
      "  \"gap_size\": 0,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 384,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"inner_group_num\": 1,\n",
      "  \"intermediate_size\": 1536,\n",
      "  \"is_decoder\": false,\n",
      "  \"is_encoder_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"min_length\": 0,\n",
      "  \"model_type\": \"albert\",\n",
      "  \"net_structure_type\": 0,\n",
      "  \"no_repeat_ngram_size\": 0,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_groups\": 1,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"num_memory_blocks\": 0,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"prefix\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"task_specific_params\": null,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "I0430 03:48:13.689114 140367158015808 modeling_utils.py:507] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/clue/albert_chinese_small/pytorch_model.bin from cache at /root/.cache/torch/transformers/a6c4ebd748ad0472ac5ca83c448fd0ebef9ea47e92ecf075d432a1d09c485176.305e2d620ca7e121bfb3dbba4945ea1547b1fb2769f2e035e8933dba9c3c33a1\n",
      "I0430 03:48:13.850312 140367158015808 modeling_utils.py:601] Weights of AlbertForQuestionAnswering not initialized from pretrained model: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "I0430 03:48:13.850879 140367158015808 modeling_utils.py:607] Weights from pretrained model not used in AlbertForQuestionAnswering: ['predictions.bias', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.decoder.bias']\n"
     ]
    }
   ],
   "source": [
    "def use_model():\n",
    "    from transformers import BertTokenizer, AlbertForQuestionAnswering\n",
    "    tokenizer = BertTokenizer.from_pretrained(\"clue/albert_chinese_small\")\n",
    "    albert = AlbertForQuestionAnswering.from_pretrained(\"clue/albert_chinese_small\")\n",
    "    return albert, tokenizer\n",
    "\n",
    "# ALBERT\n",
    "model, tokenizer = use_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 輸入(context、question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 101, 4374, 1920, 3209, 3085,  818,  784, 7938,  102, 4374, 1920, 3209,\n",
      "         3221, 3413, 7269,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "context = '王大明是校長'\n",
    "question = '王大明擔任什麼'\n",
    "input_encode = tokenizer.encode_plus(question,context,add_special_tokens=True,return_tensors='pt')\n",
    "print(input_encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segment_a_ids = (input_encode['token_type_ids'].squeeze(0) == 0).nonzero().transpose(0, 1).squeeze(0) # 找question長度\n",
    "len(segment_a_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 14\n"
     ]
    }
   ],
   "source": [
    "answer_padding = len(segment_a_ids) # 計算 [CLS]SEGMENT_A[SEP] 偏移量\n",
    "answer_start_position = 4 + answer_padding\n",
    "answer_end_position = 5 + answer_padding\n",
    "print(answer_start_position,answer_end_position)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "校 長\n"
     ]
    }
   ],
   "source": [
    "answer_start_id = input_encode['input_ids'][0][answer_start_position].item()\n",
    "answer_end_id = input_encode['input_ids'][0][answer_end_position].item()\n",
    "print(tokenizer.decode(answer_start_id),tokenizer.decode(answer_end_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([13])\n",
      "tensor([14])\n"
     ]
    }
   ],
   "source": [
    "start_position_labels = torch.tensor([answer_start_position])\n",
    "end_position_labels = torch.tensor([answer_end_position])\n",
    "print(start_position_labels)\n",
    "print(end_position_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0334, -0.3516, -0.0872, -0.1833,  0.0492,  0.0119,  0.1604,  0.2262,\n",
      "          0.1590, -0.4066, -0.2251, -0.1369, -0.3766,  0.0882, -0.1281, -0.2077]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "tensor([[ 0.4998,  0.2118,  0.3676,  0.5608,  0.3600, -0.2253, -0.1838,  0.4214,\n",
      "         -0.0127,  0.0307,  0.5352,  0.7715,  0.3497,  0.3239,  0.4949, -0.0497]],\n",
      "       grad_fn=<SqueezeBackward1>)\n"
     ]
    }
   ],
   "source": [
    "loss, start_scores, end_scores = model(input_encode['input_ids'],token_type_ids=input_encode['token_type_ids'],\\\n",
    "                                start_positions= start_position_labels, end_positions= end_position_labels )\n",
    "print(start_scores)\n",
    "print(end_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 輸出、挑選答案"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 16])\n",
      "torch.Size([1, 16])\n"
     ]
    }
   ],
   "source": [
    "print(start_scores.shape)\n",
    "print(end_scores.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7, 6, 8, 13, 4, 0, 5, 2, 14, 11]\n",
      "[11, 3, 10, 0, 14, 7, 2, 4, 12, 13]\n"
     ]
    }
   ],
   "source": [
    "predict_start_positions = _get_best_indexes(start_scores.squeeze(0),10)\n",
    "predict_end_positions = _get_best_indexes(end_scores.squeeze(0),10)\n",
    "print(predict_start_positions)\n",
    "print(predict_end_positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_scores = start_scores.squeeze(0)\n",
    "end_scores = end_scores.squeeze(0)\n",
    "answer_results=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "校長\n",
      "校\n",
      "長\n",
      "明\n",
      "明是校長\n",
      "明是\n",
      "明是校\n"
     ]
    }
   ],
   "source": [
    "for start_index in predict_start_positions:\n",
    "    for end_index in predict_end_positions:\n",
    "        answer_ids = input_encode['input_ids'].squeeze(0)[start_index:end_index+1]\n",
    "        answer_token = tokenizer.convert_ids_to_tokens(answer_ids)\n",
    "\n",
    "        if(len(answer_token) == 0 or len(answer_token)>30):\n",
    "            continue\n",
    "        elif(_check_has_skip_token(check_tokens = answer_token, skip_tokens = ['[CLS]','[SEP]','[PAD]'])):\n",
    "            continue\n",
    "        elif(_check_segment_type_is_a(start_index,end_index,input_encode['token_type_ids'].squeeze(0))):\n",
    "            continue\n",
    "        answer = \"\".join(answer_token)\n",
    "        print(answer)\n",
    "        answer_result = (start_index,start_scores[start_index].item(),end_index,end_scores[end_index].item(),answer)\n",
    "        answer_results.append(answer_result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
